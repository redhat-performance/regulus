#!/bin/bash
# This file is auto-generated.
# vim: autoindent tabstop=4 shiftwidth=4 expandtab softtabstop=4 filetype=bash
# -*- mode: sh; indent-tabs-mode: nil; sh-basic-offset: 4 -*-

source ${REG_ROOT}/system.config
source ${REG_ROOT}/lab.config
source ${REG_ROOT}/templates/common/worker_labels.config
source ${REG_ROOT}/init.sh
source ${REG_ROOT}/templates/common/remotehost_functions
source ${REG_ROOT}/templates/common/functions

DURATION=${DURATION:-0}

scale_up_factor=$TPL_SCALE_UP_FACTOR # Number of client-server pairs per host/node/node-pair.  Consider using even numbers due to NUMA carving

# Variables which apply to all test environments
################################################
topo=$TPL_TOPO # internode = client/server pods on different nodes in ocp/k8s cluster
                 # intranode = client/server pods on same worker node in ocp/k8s cluster
                 # ingress = client outside (BML host or VM), server inside ocp/k8s cluster
                 # interhost = between two BML hosts/VMs, not k8s/ocp
scale_out_factor=1   # Determines the number of hosts/nodes that will get used
                 # For internode: total workers = 2 * $scale_out_factor, and each worker-pair
                 #  consists of first worker running uperf-client pods and the second uperf-server pods.
                 # For ingress/egress: (OCP) total workers = 1 * $scale_out_factor, where uperf client and server
                 #  pods are on the same worker (no external traffic)
rm_factor=1      # Determines the number of external hosts that will get used for ingress/egress remotehost scaling
userenv=fedora-latest # can be centos7, centos8, stream, rhubi8, debian, opensuse
osruntime=chroot # can be pod or kata for OCP (not yet verified for SRIOV), chroot for remotehost


max_failures=1 # After this many failed samples=$NUM_SAMPLE the run will quit
user_tags="datapath:$REG_DP,topo:$TPL_TOPO" # Comma-separated list of something=value, these help you identify this run as different
            #  from other runs, for example:  "cloud-reservation:48,HT:off,CVE:off"
            # Note that many tags are auto-generated below
mv_params_file=mv-params.json # All benchmark-iterations are built from this file

# Variables for ocp/k8s environments
####################################
num_cpus=${USEABLE_CPUS}  # A few fewer than the number of *Allocatable* cpus on each of the workers.
             # as reported by oc describe node/node-name
             # This is  automatically calculated by lab-analyzer
pod_qos=burstable # static = guaranteed pod, burstable = default pos qos
ocphost=$REG_OCPHOST
k8susr=$REG_KNI_USER # Might be "root" or "kni" for some installations


if [ "$TPL_PAO" == 1 ] || [ "$TPL_SRIOV" == 1 ] || [ "$TPL_HOSTNETWORK" == 1 ]; then
    securityContext_file="`/bin/pwd`/securityContext.json"
fi

if [ "$TPL_PAO" == 1 ] || [ "$TPL_SRIOV" == 1 ] || [ "$TPL_MACVLAN" == 1 ]; then
    annotations="`/bin/pwd`/annotations.json"  &> /dev/null
fi
if [ "$TPL_PAO" == 1 ] ; then
    runtimeClassNameOpt=",runtimeClassName:performance-${MCP}"
fi

if [ "$TPL_SRIOV" == 1 ] ; then
    source ${REG_ROOT}/SRIOV-config/config.env
    do_ssh $k8susr@$ocphost "kubectl delete ns $OCP_PROJECT" &> /dev/null
    do_ssh $k8susr@$ocphost "kubectl create ns $OCP_PROJECT"

    # crucible delete NS and thus also delete neworkAttachmentDefinition. Now we need to recreate them.
    do_ssh $k8susr@$ocphost "kubectl config set-context --current --namespace=$OCP_PROJECT && kubectl apply -f ${REM_REG_ROOT}/${REG_SRIOV_NAD}"
    #KNI_HOME=$(do_ssh $k8susr@$ocphost "pwd")
    #do_ssh $k8susr@$ocphost "kubectl apply -f ${KNI_HOME}/${REG_SRIOV_NAD}"

    # sometime network-attachment takes a few seconds to be available
    sleep 5
fi

if [ "$TPL_MACVLAN" == 1 ] ; then
  	source ${REG_ROOT}/MACVLAN-config/config.env
    do_ssh $k8susr@$ocphost "kubectl delete ns $OCP_PROJECT" &> /dev/null
    do_ssh $k8susr@$ocphost "kubectl create ns $OCP_PROJECT"

    # crucible delete NS and thus also delete neworkAttachmentDefinition. Now we need to recreate them.
    do_ssh $k8susr@$ocphost "kubectl config set-context --current --namespace=$OCP_PROJECT && kubectl apply -f ${REM_REG_ROOT}/${REG_MACVLAN_NAD}"
    # sometime network-attachment takes a few seconds to be available
    sleep 5
fi

# Use to disable or enable IRQs, comment out if you are not using Performance Addon Operator
irq="bal" # bal by default or rrHost or <something-else> depending on what manual mods made
          # This is completely manual and needs to be confirmed by the user!

# Variables if one or more remotehost
# endpoints are used (topo=ingress|egress|interhost)
#####################################
read -a BMLHOSTS <<< "$BM_HOSTS"
bmlhosta=${BMLHOSTS[0]}
bmlhostb=${BMLHOSTS[1]}

if [ -f "${TPL_RESOURCES}" ]; then
   resource_file="`/bin/pwd`/${TPL_RESOURCES}"
else
   resource_file="`/bin/pwd`/resource.json"
fi
    
# Create a resource JSON to size the pods
function gen_resource_file() {
    local cpu=$1
    if [ "$pod_qos" == "static" ]; then
        # static has number of cpu constraints.
        #   1. multiple number of cores i.e. 2,4,6 and cannot be 1,3,5,7 when HT is on
        #   2. if 2 CPUs per core (HT), each 2 CPU must be on the same core, In other words, the
        #      number of CPUs cannot be great than the numbe of cores*2. And we know some
        #      CPUs have been used the system. So we just make a best guess available cores=cores-3
        
        # strip the last char 'm', and convert to num of whole CPU 
        local stripped_cpu="${cpu%?}"
        local whole=$(( stripped_cpu / 1000 ))
        # divisable by 2, since mostly we have HT on. So gu CPU must be full core.
        cpu=$(( whole / 2 * 2 ))  

        # if scale factor= 1, and number of cpu is greater than 2 * (CORE_PERSOCKET - 3) will hit not enough resource.
        if [ $scale_up_factor == 1 ]; then
            max=$(( (CORE_PERSOCKET - 3) * 2 ))
            if [ $cpu > $max ]; then
                cpu=$max
            fi
        fi

        echo '"resources": {'     >$resource_file
        echo '    "requests": {' >>$resource_file
        # TODO: horribly broken! Need to round down to whole number!
        echo '        "cpu": "'$cpu'",' >>$resource_file
        echo '        "memory": "2048Mi"' >>$resource_file
        echo '    },' >>$resource_file
        echo '    "limits": {' >>$resource_file
        # TODO: horribly broken! Need to round down to whole number!
        echo '        "cpu": "'$cpu'",' >>$resource_file
        echo '        "memory": "2048Mi"' >>$resource_file
        echo '    }' >>$resource_file
        echo '}' >>$resource_file
    else # non-guaranteed pod
        echo '"resources": {'     >$resource_file
        echo '    "requests": {' >>$resource_file
        echo '        "cpu": "'$cpu'"' >>$resource_file
        echo '    }' >>$resource_file
        echo '}' >>$resource_file
    fi
}


# check for dependencies
bins="jq bc do_ssh sed crucible"
missing_bins=""
for bin in $bins; do
    which $bin >/dev/null 2>&1 || missing_bins="$missing_bins $bin"
done
if [ ! -z "$missing_bins" ]; then
    echo "ERROR: these required bins are needed; please install before running this script:"
    echo $missing_bins
    exit 1
fi
    

# What is below is code to generate the appropriate crucible command to tun your test.
# There is a ton of duplicated code that needs to be consolidated.

if [ ! -z "$annotations" ]; then
    if [ -f "$annotations" ]; then
        echo "Using annotations: $annotations"
        anno_opt=",annotations:default:$annotations"
    else
        echo "Annoations file missing: $annotations"
        exit
    fi
else
    anno_opt=""
fi

if [ "$DURATION" != 0 ]; then
	# There is env-defined duration. Use it.
	FILES=$(ls *mv*.json)
	for file in $FILES; do
       	sed -i 's/\({ "arg": "time", "vals": \["\)[0-9]*\("\] }\)/\1'"$DURATION"'\2/g' $file
   	done
fi

for num_pods in $scale_up_factor; do
    num_clients=`echo "$num_pods * $scale_out_factor" | bc`
    num_servers=$num_clients
    if [ "$topo" != "interhost" ]; then # Any test involving ocp/k8s
        do_ssh $k8susr@$ocphost "kubectl get nodes -o json" >nodes.json
        # support one MATCH label and 4 MATCH_NOT labels for workers selection
        if [ -z "$MATCH" ]; then
            my_select_var=".\"node-role.kubernetes.io/worker\" != null"
        else
            my_select_var=".\"node-role.kubernetes.io/$MATCH\" != null"
        fi
        if [ -n "$MATCH_NOT" ]; then
            my_select_var+=" and .\"node-role.kubernetes.io/$MATCH_NOT\" == null"
        fi
        workers=($(jq -r ".items[] | .metadata.labels | select($my_select_var) | .\"kubernetes.io/hostname\"" nodes.json))
        #echo workers: "${workers[@]:0:3}"
        DPRINT $LINENO "workers=${workers[@]}"

        first_worker=`echo ${workers[0]}`
        if [ -z "$first_worker" ]; then
            "First worker not defined, exiting"
            exit 1
        fi
        if [ "$topo" == "internode" ]; then
            nodes_per_client_server=2
        else
            nodes_per_client_server=1
        fi
        min_worker_nodes=`echo "$scale_out_factor * $nodes_per_client_server" | bc`
        if [ ${#workers[@]} -lt $min_worker_nodes ]; then
            echo "Need at least $min_worker_nodes to run tests, and this cluster only has ${#workers[@]}"
            exit 1
        fi
        if [ "$topo" == "intranode" ]; then
            per_pod_cpu=`echo "1000 * $num_cpus / $num_pods / 2" | bc`m
        else
            per_pod_cpu=`echo "1000 * $num_cpus / $num_pods" | bc`m
        fi
        if [  -f "${TPL_RESOURCES}" ];then
            echo "User-provided: $resource_file"
        else
            echo "Computing resource file: $resource_file"
            gen_resource_file $per_pod_cpu
        fi
        do_ssh $k8susr@$ocphost "kubectl get nodes/$first_worker -o json" >worker.json
        kernel=`jq -r .status.nodeInfo.kernelVersion worker.json`
        rcos=`jq -r .status.nodeInfo.osImage  worker.json | awk -F"CoreOS " '{print $2}' | awk '{print $1}'`
        do_ssh $k8susr@$ocphost "kubectl get networks.operator.openshift.io cluster -o yaml" >networks-operator.json
        do_ssh $k8susr@$ocphost "kubectl get network -o json" >network.json
        network_type=`jq -r .items[0].status.networkType network.json`
        network_mtu=`jq -r .items[0].status.clusterNetworkMTU network.json`
            ns_file[0]=""
            ns_file[1]=""
            # Create a nodeSelector JSON to place pods
            for i in `seq 1 $scale_out_factor`; do
                for j in 0 $(($nodes_per_client_server-1)); do
                    if [ "$topo" == "ingress" -o "$topo" == "egress" ]; then
                       idx=`echo "($i-1)" | bc`
                    else
                       idx=`echo "($i-1)*2+$j" | bc`
                    fi
                    this_worker=${workers[$idx]}
                    ns_file[$j]=`/bin/pwd`/nodeSelector-$this_worker.json
                    echo '"nodeSelector": {' >${ns_file[$j]}
                        echo '    "kubernetes.io/hostname": "'$this_worker'"' >>${ns_file[$j]}
                    echo '}' >>${ns_file[$j]}
                done
                # Populate the node_selector option
                for k in `seq 1 $num_pods`; do
                    if [ "$topo" == "internode" -o "$topo" == "egress" ]; then
                        client_idx=0
                        server_idx=1
                    fi
                    if [ "$topo" == "intranode" -o "$topo" == "ingress" ]; then
                        client_idx=0
                        server_idx=0
                    fi
                    cs_num=`echo "$num_pods * ($i-1) + $k" | bc`
                    # Add clients
                    if [ "$topo" == "internode" -o "$topo" == "intranode" -o "$topo" == "egress" ]; then
                        node_selector="$node_selector,`printf "nodeSelector:client-$cs_num:\${ns_file[$client_idx]}"`"
                    fi
                    # Add servers
                    if [ "$topo" == "internode" -o "$topo" == "intranode" -o "$topo" == "ingress" ]; then
                        node_selector="$node_selector,`printf "nodeSelector:server-$cs_num:\${ns_file[$server_idx]}"`"
                    fi
                done
                node_selector=`echo $node_selector | sed -e s/^,//`
            done
        endpoint_opt="--endpoint k8s,user:$k8susr,host:$ocphost"
        if  [  ! -z ${OCP_PROJECT} ]; then
            endpoint_opt+=",unique-project:${OCP_PROJECT}"
        fi
        endpoint_opt+=",${node_selector}"
        endpoint_opt+=",userenv:$userenv"
        endpoint_opt+=",resources:default:$resource_file"
        endpoint_opt+=",osruntime:${osruntime}"
        endpoint_opt+="$anno_opt"
        endpoint_opt+="${runtimeClassNameOpt}"
        if [ ! -z "$securityContext_file" ]; then
            if [ -f "$securityContext_file" ]; then
                endpoint_opt+=",securityContext:default:$securityContext_file"
            fi
        fi
        if [ -n "$TPL_LBSVC" ]; then
            endpoint_opt+=",lbSvc:$TPL_LBSVC"
        fi
    else
        echo "interhost"
        endpoint_opt=""
        network_type=flat
        network_mtu=8900 # TODO: get actual mtu
        rcos=na
        kernel=`do_ssh $bmlhosta uname -r`
    fi

    if [ "$topo" == "internode" -o "$topo" == "intranode" ]; then
        endpoint_opt+=",client:1-$num_clients,server:1-$num_servers"
    elif [ "$topo" == "ingress" ]; then
        # EP and svc may stick around from the last abort/CTL-C. If so, delete it by hand in debug.
        endpoint_opt+=",server:1-$num_servers"
        if [ -n "$rm_factor" ] && [ "$rm_factor" -gt 1 ]; then
            multi_rm_hosts "client"
        else
            endpoint_opt+=" --endpoint remotehost,user:root,host:$bmlhosta,client:1-$num_pods,userenv:$userenv "
        fi
    elif [ "$topo" == "egress" ]; then
        endpoint_opt+=",client:1-$num_servers"
        if [ -n "$rm_factor" ] && [ "$rm_factor" -gt 1 ]; then
            multi_rm_hosts "server"
            # func sourced from remotehost_functions
        else
            endpoint_opt+=" --endpoint remotehost,user:root,host:$bmlhosta,server:1-$num_pods,userenv:$userenv "
        fi
    elif [ "$topo" == "interhost" ]; then
        # TODO: make work for $scale_out_factor > 1
        endpoint_opt+=" --endpoint remotehost,user:root,host:$bmlhosta,client:1-$num_pods,userenv:$userenv,osruntime:$osruntime"
        endpoint_opt+=" --endpoint remotehost,user:root,host:$bmlhostb,server:1-$num_pods,userenv:$userenv,osruntime:$osruntime"
    fi

    tags="sdn:$network_type,mtu:$network_mtu,rcos:$rcos,kernel:$kernel,irq:$irq,userenv:$userenv,osruntime:$osruntime"
    tags+=",topo:$topo,pods-per-worker:$num_pods,scale_out_factor:$scale_out_factor"
    if [ "$topo" == "internode" -a "$topo" == "intranode" ]; then
        tags+=",pod_qos:$pod_qos"
    fi
    if [ ! -z "$other_tags" ]; then
        tags+="other_tags"
    fi
    if [ "$DRY_RUN" == true  ]; then
        echo DRY=true
        endpoint_opt+=",dry-run:true"
    fi

    if [ "$TPL_HOSTNETWORK" == 1 ]; then
       endpoint_opt+=",hostNetwork:1"
    fi

    do_ssh $k8susr@$ocphost "kubectl delete ns $OCP_PROJECT" &> /dev/null
    do_ssh $k8susr@$ocphost "kubectl create ns $OCP_PROJECT && \
                 kubectl label ns $OCP_PROJECT pod-security.kubernetes.io/enforce=privileged \
                      pod-security.kubernetes.io/enforce-version=v1.27 --overwrite" 

ARGS=" --tags $tags \
        --mv-params $mv_params_file \
        --num-samples=$NUM_SAMPLES  \
        --max-sample-failures=$max_failures \
        $endpoint_opt \
        --max-rb-attempts=3"

    if [ -z "$ONEJSON" ]; then
        time crucible run iperf $ARGS
    else
        reg-c2j-config.py --name iperf --output all-in-one.json --tool-params tool-params.json  $ARGS && \
        time crucible run --from-file all-in-one.json
    fi

done
