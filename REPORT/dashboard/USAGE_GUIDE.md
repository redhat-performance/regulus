# Dashboard Usage Guide

Complete guide to using the Performance Benchmark Dashboard.

## Table of Contents
1. [Getting Started](#getting-started)
2. [Dashboard Interface](#dashboard-interface)
3. [Common Workflows](#common-workflows)
4. [API Reference](#api-reference)
5. [Tips and Tricks](#tips-and-tricks)

## Getting Started

### First Time Setup

1. **Install Flask**
   ```bash
   pip install flask
   ```

2. **Generate Reports**
   ```bash
   cd /path/to/build_report
   ./build_report --formats json --output /tmp/my-report
   ```

3. **Launch Dashboard**
   ```bash
   python3 dashboard/run_dashboard.py --reports /tmp
   ```

4. **Open Browser**
   - Navigate to http://localhost:5000
   - Or use your server IP: http://YOUR_IP:5000

### Understanding the Data

The dashboard loads **JSON reports** generated by the build_report tool. Each report contains:
- **Results**: Performance test results from result-summary.txt files
- **Iterations**: Multiple test runs with different parameters
- **Metrics**: Mean, min, max, stddev, CPU utilization
- **Tags**: Model, kernel, topology, performance baseline
- **Timestamps**: When the report was generated

## Dashboard Interface

### Navigation Bar

```
┌─────────────────────────────────────────────────────┐
│ Performance Benchmark Dashboard    [Reload Reports] │
└─────────────────────────────────────────────────────┘
```

**Reload Reports Button**: Click to refresh data without restarting the server.

### Summary Cards

```
┌──────────────┬──────────────┬──────────────┬──────────────┐
│ Total        │ Total        │ Benchmark    │ Date Range   │
│ Reports      │ Results      │ Types        │              │
│      5       │     125      │      3       │ Jan - Feb    │
└──────────────┴──────────────┴──────────────┴──────────────┘
```

Quick overview of loaded data.

### Filters Section

```
┌─────────────────────────────────────────────────────────┐
│ Filters                                                 │
├─────────────────────────────────────────────────────────┤
│ [Benchmark ▼] [Model ▼] [Kernel ▼] [Topo ▼] [Perf ▼]  │
│                                      [Clear Filters]    │
└─────────────────────────────────────────────────────────┘
```

**How Filters Work:**
- Filters are **global** - they apply to all tabs
- Filters are **cumulative** - multiple filters narrow down results
- Filters **persist** when switching tabs
- Click "Clear Filters" to reset all

**Example:**
```
Benchmark: uperf    → Shows only uperf results
Model: e810         → Further filters to e810 model
Kernel: 5.14        → Shows only kernel 5.14
Result: Only uperf tests on e810 with kernel 5.14
```

### Tabs

```
┌─────────┬────────┬────────────┬──────────────┐
│Overview │ Trends │ Comparison │ Results Table│
└─────────┴────────┴────────────┴──────────────┘
```

## Tab Details

### 1. Overview Tab

**Purpose:** Get a quick summary of performance across configurations.

**Components:**

**A. Performance by Model** (Bar Chart)
- Shows average throughput for each network adapter model
- Hover over bars to see exact values
- Good for comparing different hardware

**B. Performance by Kernel** (Bar Chart)
- Shows average throughput for each kernel version
- Helps identify performance regressions or improvements
- Useful for kernel upgrade decisions

**C. Top 10 Performers** (Table)
- Lists the best performing test configurations
- Sorted by mean throughput (highest first)
- Shows: Rank, Benchmark, Model, Kernel, Topology, Mean, Unit, CPU%

**Use Cases:**
- Quick health check of test results
- Identify best performing configurations
- Find which hardware/kernel combinations work best

### 2. Trends Tab

**Purpose:** Track performance changes over time.

**Controls:**
```
Performance Trends Over Time          [Group by: Model ▼]
```

**Grouping Options:**
- **No Grouping**: All results in one line
- **Group by Model**: Separate line for each model
- **Group by Kernel**: Separate line for each kernel version
- **Group by Topology**: Separate line for each topology

**How to Use:**

1. **Detect Regressions:**
   ```
   Select "Group by Kernel"
   Look for downward trends
   → Identifies kernel versions with performance drops
   ```

2. **Track Improvements:**
   ```
   Select "Group by Model"
   Compare lines over time
   → See which models improved/degraded
   ```

3. **Monitor Stability:**
   ```
   Select "No Grouping"
   Look for variance in the line
   → High variance = unstable performance
   ```

**Example Workflow: Finding a Performance Regression**

1. Launch dashboard with historical reports:
   ```bash
   python3 dashboard/run_dashboard.py --reports /data/historical
   ```

2. Go to Trends tab
3. Select "Group by Kernel"
4. Look for downward slopes
5. Click on data points to see exact values
6. Cross-reference with kernel version dates

### 3. Comparison Tab

**Purpose:** Compare two configurations side-by-side.

**Interface:**
```
┌───────────────────────────────────────────────────────┐
│ Compare Field: [Model ▼]                              │
│ Value A: [e810 ▼]                                     │
│ Value B: [e910 ▼]                                     │
│                                    [Compare]          │
├───────────────────────────────────────────────────────┤
│ Comparison Results:                                   │
│                                                       │
│ model=e810              │  model=e910                │
│ 95.2 Gbps              │  105.4 Gbps                │
│                                                       │
│ Difference: +10.2 Gbps (+10.7%)                      │
│ Better Configuration: model=e910                     │
└───────────────────────────────────────────────────────┘
```

**Compare Fields:**
- **Model**: Different network adapters
- **Kernel**: Different kernel versions
- **Topology**: Different network topologies
- **Performance**: Baseline vs tuned configurations

**How to Use:**

1. **Select Compare Field**: What aspect to compare
2. **Select Value A**: First configuration
3. **Select Value B**: Second configuration
4. **Click Compare**: View results

**Reading Results:**
- **Green box**: Better performing configuration
- **Percentage**: Relative improvement
- **Difference**: Absolute throughput difference

**Example: Hardware Evaluation**
```
Goal: Should we upgrade from e810 to e910?

Steps:
1. Compare Field: Model
2. Value A: e810
3. Value B: e910
4. Click Compare

Result:
e910 is 15% faster
Decision: Upgrade justified if cost-effective
```

### 4. Results Table Tab

**Purpose:** Browse detailed results with search and sort.

**Features:**

**Search Box:**
```
Search results: [____________]    Showing 1 to 25 of 150 entries
```
- Type to filter results in real-time
- Searches across all columns
- Case-insensitive

**Sortable Columns:**
- Click column headers to sort
- Click again to reverse sort order
- Default: Sorted by mean throughput (descending)

**Pagination:**
```
[Previous] 1 2 3 4 5 [Next]
```
- 25 results per page
- Change in DataTables settings

**Columns:**
- **Benchmark**: Test type (uperf, iperf, trafficgen)
- **Model**: Network adapter model
- **Kernel**: Kernel version
- **Topology**: Network topology
- **Protocol**: TCP/UDP
- **Test Type**: Stream, RR, etc.
- **Mean**: Average throughput
- **StdDev**: Standard deviation
- **CPU %**: CPU utilization
- **Timestamp**: When test ran

**Use Cases:**

1. **Find Specific Test:**
   ```
   Search: "e810 5.14 tcp"
   → Shows all e810 tests on kernel 5.14 with TCP
   ```

2. **Find Best CPU Efficiency:**
   ```
   Click "CPU %" column header
   → Sorted by lowest CPU usage
   Find tests with high throughput + low CPU
   ```

3. **Export Data:**
   ```
   Copy table data to clipboard
   Or use API: curl http://localhost:5000/api/results
   ```

## Common Workflows

### Workflow 1: Weekly Performance Review

**Goal:** Check if this week's performance is on par with previous weeks.

```bash
# 1. Collect all weekly reports
mkdir /tmp/weekly-reports
cp week1-report.json /tmp/weekly-reports/
cp week2-report.json /tmp/weekly-reports/
cp week3-report.json /tmp/weekly-reports/

# 2. Launch dashboard
python3 dashboard/run_dashboard.py --reports /tmp/weekly-reports

# 3. In browser:
#    - Go to Trends tab
#    - Select "No Grouping"
#    - Look for downward trends
#    - If performance dropped, investigate further
```

### Workflow 2: Hardware Comparison

**Goal:** Compare two network adapter models.

```bash
# 1. Generate reports for both
./build_report --formats json --output /tmp/e810-report  # From e810 tests
./build_report --formats json --output /tmp/e910-report  # From e910 tests

# 2. Put in same directory
mkdir /tmp/comparison
cp /tmp/e810-report.json /tmp/comparison/
cp /tmp/e910-report.json /tmp/comparison/

# 3. Launch dashboard
python3 dashboard/run_dashboard.py --reports /tmp/comparison

# 4. In browser:
#    - Go to Comparison tab
#    - Compare Field: Model
#    - Value A: e810
#    - Value B: e910
#    - Click Compare
#    - Review percentage difference
```

### Workflow 3: Kernel Regression Testing

**Goal:** Verify new kernel doesn't regress performance.

```bash
# 1. Before kernel upgrade
./build_report --formats json --output /tmp/kernel-5.14

# 2. After kernel upgrade
./build_report --formats json --output /tmp/kernel-5.15

# 3. Launch dashboard
mkdir /tmp/kernel-test
cp /tmp/kernel-5.14.json /tmp/kernel-test/
cp /tmp/kernel-5.15.json /tmp/kernel-test/
python3 dashboard/run_dashboard.py --reports /tmp/kernel-test

# 4. In browser:
#    - Go to Comparison tab
#    - Compare Field: Kernel
#    - Value A: 5.14
#    - Value B: 5.15
#    - Click Compare
#    - If 5.15 is worse, investigate
```

## API Reference

All dashboard data is accessible via REST API.

### Base URL
```
http://localhost:5000/api
```

### Endpoints

#### GET /api/summary
**Returns:** Overall statistics

```bash
curl http://localhost:5000/api/summary
```

**Response:**
```json
{
  "reports": {
    "total_reports": 3,
    "total_iterations": 25,
    "benchmarks": ["uperf", "iperf"],
    "date_range": {
      "earliest": "2025-01-15T10:00:00",
      "latest": "2025-02-20T15:30:00"
    }
  },
  "benchmarks": {
    "total_results": 25,
    "benchmarks": {"uperf": 20, "iperf": 5}
  }
}
```

#### GET /api/results
**Returns:** All results with optional filtering

**Parameters:**
- `benchmark` - Filter by benchmark type
- `model` - Filter by model
- `kernel` - Filter by kernel version
- `topo` - Filter by topology
- `perf` - Filter by performance baseline

```bash
# All results
curl http://localhost:5000/api/results

# Filtered
curl "http://localhost:5000/api/results?benchmark=uperf&model=e810"
```

#### GET /api/trends
**Returns:** Time-series trend data

**Parameters:**
- `metric` - Metric to track (default: mean)
- `group_by` - Group by field (model, kernel, topo)
- `benchmark` - Filter to benchmark type

```bash
curl "http://localhost:5000/api/trends?group_by=model&benchmark=uperf"
```

#### GET /api/compare
**Returns:** Side-by-side comparison

**Parameters:**
- `field` - Field to compare (model, kernel, topo, perf)
- `value_a` - First value
- `value_b` - Second value
- `metric` - Metric to compare (default: mean)
- `benchmark` - Filter to benchmark type

```bash
curl "http://localhost:5000/api/compare?field=model&value_a=e810&value_b=e910"
```

#### GET /api/statistics
**Returns:** Statistics grouped by field

**Parameters:**
- `group_by` - Group by field (default: model)
- `metric` - Metric to analyze (default: mean)
- `benchmark` - Filter to benchmark type

```bash
curl "http://localhost:5000/api/statistics?group_by=kernel&metric=mean"
```

#### GET /api/top_performers
**Returns:** Top N performing results

**Parameters:**
- `metric` - Metric to rank by (default: mean)
- `top_n` - Number of results (default: 10)
- `benchmark` - Filter to benchmark type
- `ascending` - true for lowest, false for highest (default: false)

```bash
curl "http://localhost:5000/api/top_performers?top_n=5&benchmark=uperf"
```

#### GET /api/filters
**Returns:** Available filter options

```bash
curl http://localhost:5000/api/filters
```

**Response:**
```json
{
  "benchmark": ["uperf", "iperf", "trafficgen"],
  "model": ["e810", "e910", "cx5"],
  "kernel": ["5.14", "5.15", "5.16"],
  "topo": ["linear", "mesh", "star"]
}
```

#### POST /api/reload
**Returns:** Success/error status

**Body:** (optional)
```json
{
  "reports_dir": "/new/path/to/reports"
}
```

```bash
curl -X POST http://localhost:5000/api/reload \
     -H "Content-Type: application/json" \
     -d '{"reports_dir": "/tmp/new-reports"}'
```

## Tips and Tricks

### Tip 1: Use Debug Mode During Development
```bash
python3 dashboard/run_dashboard.py --reports . --debug
```
- Auto-reloads on code changes
- Shows detailed error messages
- Useful for troubleshooting

### Tip 2: Access from Another Machine
```bash
# Dashboard binds to 0.0.0.0 by default
# Find your server IP:
ip addr show | grep "inet "

# Access from another machine:
http://YOUR_SERVER_IP:5000
```

### Tip 3: Use Filters to Focus
- Start broad, narrow down with filters
- Example: All results → Filter to uperf → Filter to e810 → Filter to kernel 5.14
- Click "Clear Filters" to start over

### Tip 4: Compare Apples to Apples
- When comparing, use filters to ensure fair comparison
- Example: When comparing models, filter to same kernel version
- Results are averaged, so more data = better comparison

### Tip 5: Export Data for Further Analysis
```bash
# Get all results as JSON
curl http://localhost:5000/api/results > all_results.json

# Process with jq
curl http://localhost:5000/api/results | jq '.[] | select(.mean > 100)'
```

### Tip 6: Monitor Multiple Report Directories
```bash
# Create symbolic links
mkdir /tmp/all-reports
ln -s /data/weekly-reports/*.json /tmp/all-reports/
ln -s /data/regression-tests/*.json /tmp/all-reports/

python3 dashboard/run_dashboard.py --reports /tmp/all-reports
```

### Tip 7: Use SSH Tunneling for Remote Access
```bash
# From your local machine:
ssh -L 5000:localhost:5000 user@remote-server

# Then access at:
http://localhost:5000
```

### Tip 8: Automate Report Generation
```bash
#!/bin/bash
# daily_dashboard.sh

# Generate today's report
./build_report --formats json --output /data/reports/report-$(date +%Y%m%d)

# Dashboard auto-reloads with "Reload Reports" button
# Or restart dashboard to pick up new reports
```

## Best Practices

1. **Regular Reports**: Generate reports regularly for meaningful trends
2. **Consistent Tags**: Use consistent model/kernel/topo tags across tests
3. **Multiple Iterations**: Run multiple test iterations for accurate averages
4. **Document Changes**: Note what changed between test runs
5. **Archive Reports**: Keep historical reports for long-term trend analysis
6. **Use Filters Wisely**: Start broad, narrow down as needed
7. **Cross-Reference**: Use multiple tabs to validate findings
8. **API for Automation**: Use API endpoints in scripts for automated analysis

## Need Help?

- **Documentation**: See README.md and QUICKSTART.md
- **Troubleshooting**: See Troubleshooting section in README.md
- **API Reference**: This document, API Reference section
- **Examples**: See dashboard/QUICKSTART.md for workflows
