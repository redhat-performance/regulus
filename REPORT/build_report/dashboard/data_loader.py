"""
Data loader for dashboard - loads and parses JSON reports.

Reads JSON reports generated by build_report tool and provides
structured access to the performance data.
"""

import json
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime
import re


@dataclass
class BenchmarkResult:
    """Single benchmark result from a test iteration."""
    regulus_data: str
    benchmark: str
    iteration_id: str
    test_type: Optional[str] = None
    protocol: Optional[str] = None

    # Key tags
    model: Optional[str] = None  # Datapath model: OVNK, DPU, MACVLAN, SRIOV
    nic: Optional[str] = None    # NIC vendor: e810, e910, cx5, cx7, etc.
    arch: Optional[str] = None   # CPU architecture: emerald_rapid, sapphire_rapids, etc.
    perf: Optional[str] = None
    offload: Optional[str] = None
    kernel: Optional[str] = None
    rcos: Optional[str] = None
    cpu: Optional[str] = None
    topo: Optional[str] = None
    pods_per_worker: Optional[str] = None
    scale_out_factor: Optional[str] = None

    # Test parameters
    threads: Optional[int] = None
    wsize: Optional[int] = None
    rsize: Optional[int] = None

    # Metrics
    mean: Optional[float] = None
    min: Optional[float] = None
    max: Optional[float] = None
    stddev: Optional[float] = None
    stddevpct: Optional[float] = None
    unit: Optional[str] = None
    busy_cpu: Optional[float] = None
    samples_count: Optional[int] = None

    # Metadata
    timestamp: Optional[str] = None
    run_id: Optional[str] = None
    report_source: Optional[str] = None  # Path to the report.json file


@dataclass
class ReportMetadata:
    """Metadata about a loaded report."""
    regulus_data: str
    total_results: int
    benchmarks: List[str]
    timestamp: str
    total_iterations: int
    total_files: int


class ReportLoader:
    """Loads and parses JSON reports from build_report tool."""

    def __init__(self):
        self.loaded_reports: List[Dict[str, Any]] = []
        self.metadata: List[ReportMetadata] = []

    def load_report(self, report_path: str) -> Optional[Dict[str, Any]]:
        """
        Load a single JSON report.

        Args:
            report_path: Path to the JSON report file

        Returns:
            Parsed report data or None if loading failed
        """
        try:
            path = Path(report_path)
            if not path.exists():
                print(f"Report file not found: {report_path}")
                return None

            with open(path, 'r', encoding='utf-8') as f:
                # Allow NaN, Infinity, and -Infinity values in JSON
                # Convert them to None (null) for compatibility
                report_data = json.load(f, parse_constant=lambda x: None if x in ('NaN', '-NaN') else float(x))

            # Validate basic structure
            if not isinstance(report_data, dict):
                print(f"Invalid report format (not a dictionary): {report_path}")
                return None

            # Handle nested structure (wrapped reports with lab_info/testbed_info)
            # If 'results' is a dict containing the actual report, unwrap it but preserve lab_info
            if 'results' in report_data and isinstance(report_data['results'], dict):
                nested = report_data['results']
                if 'results' in nested and isinstance(nested['results'], list):
                    print(f"Detected nested report structure in {path.name}, unwrapping...")
                    # Preserve lab_info and testbed_info from outer level
                    if 'lab_info' in report_data:
                        nested['lab_info'] = report_data['lab_info']
                    if 'testbed_info' in report_data:
                        nested['testbed_info'] = report_data['testbed_info']
                    report_data = nested

            # Extract metadata
            gen_info = report_data.get('generation_info', {})
            if not isinstance(gen_info, dict):
                gen_info = {}

            results = report_data.get('results', [])
            if not isinstance(results, list):
                results = []

            # Count total iterations across all files
            total_iterations = 0
            for r in results:
                if isinstance(r, dict):
                    iterations = r.get('iterations', [])
                    if isinstance(iterations, list):
                        total_iterations += len(iterations)

            metadata = ReportMetadata(
                regulus_data=str(path),
                total_results=gen_info.get('total_results', len(results)),
                benchmarks=gen_info.get('benchmarks', []),
                timestamp=gen_info.get('timestamp', ''),
                total_iterations=total_iterations,
                total_files=len(results)
            )

            self.loaded_reports.append(report_data)
            self.metadata.append(metadata)

            return report_data

        except json.JSONDecodeError as e:
            print(f"Error parsing JSON from {report_path}: {e}")
            return None
        except Exception as e:
            import traceback
            print(f"Error loading report {report_path}: {e}")
            print(f"Traceback: {traceback.format_exc()}")
            return None

    def load_multiple_reports(self, report_paths: List[str]) -> List[Dict[str, Any]]:
        """
        Load multiple JSON reports.

        Args:
            report_paths: List of paths to JSON report files

        Returns:
            List of successfully loaded reports
        """
        loaded = []
        for path in report_paths:
            report = self.load_report(path)
            if report:
                loaded.append(report)
        return loaded

    def load_from_directory(self, directory: str, pattern: str = "*.json") -> List[Dict[str, Any]]:
        """
        Load all JSON reports from a directory.

        Args:
            directory: Directory containing JSON reports
            pattern: Glob pattern for matching files (default: *.json)

        Returns:
            List of successfully loaded reports
        """
        dir_path = Path(directory)
        if not dir_path.exists() or not dir_path.is_dir():
            print(f"Directory not found: {directory}")
            return []

        json_files = list(dir_path.glob(pattern))
        # Filter out schema files
        json_files = [f for f in json_files if not f.name.endswith('_schema.json')]

        print(f"Found {len(json_files)} JSON report files in {directory}")
        return self.load_multiple_reports([str(f) for f in json_files])

    def _extract_nic_mapping_from_lab_info(self, lab_info: Dict[str, Any]) -> Dict[str, str]:
        """
        Extract NIC model mapping from lab_info section.

        Returns mapping of datapath model -> NIC model
        Example: {'OVNK': 'E810', 'DPU': 'CX6', 'SRIOV': 'E810'}
        """
        mapping = {}

        # Map lab_info keys to datapath models
        # Note: Each NIC model may map to multiple datapath model variants
        nic_mappings = {
            'REG_OVN_NIC_MODEL': ['OVNK', 'OVN'],
            'REG_SRIOV_NIC_MODEL': ['SRIOV', 'SR-IOV'],
            'REG_DPDK_NIC_MODEL': ['DPU', 'DPDK'],
            'REG_MACVLAN_NIC_MODEL': ['MACVLAN'],
        }

        for lab_key, datapath_models in nic_mappings.items():
            if lab_key in lab_info:
                nic_model = lab_info[lab_key]
                if nic_model:
                    # Map this NIC model to all variant names
                    for datapath_model in datapath_models:
                        mapping[datapath_model] = str(nic_model)

        return mapping

    def extract_benchmark_results(self, report_data: Dict[str, Any], report_source: Optional[str] = None) -> List[BenchmarkResult]:
        """
        Extract individual benchmark results from a report.

        Flattens the nested structure into individual test iterations
        for easier analysis and visualization.

        Args:
            report_data: Loaded report data
            report_source: Path to the report.json file (for tracking which report the results came from)

        Returns:
            List of BenchmarkResult objects, one per test iteration
        """
        benchmark_results = []

        results = report_data.get('results', [])
        gen_info = report_data.get('generation_info', {})
        report_timestamp = gen_info.get('timestamp', '') if isinstance(gen_info, dict) else ''

        # Extract NIC mapping from lab_info if available
        lab_info = report_data.get('lab_info', {})
        nic_mapping = {}
        if isinstance(lab_info, dict):
            nic_mapping = self._extract_nic_mapping_from_lab_info(lab_info)
            if nic_mapping:
                print(f"  NIC mapping from lab_info: {nic_mapping}")

        for file_result in results:
            # Skip if not a dictionary
            if not isinstance(file_result, dict):
                continue
            # Support both 'file_path' (old) and 'regulus_data' (new) field names
            regulus_data = file_result.get('regulus_data', '') or file_result.get('file_path', '')
            benchmark = file_result.get('benchmark', '')
            run_id = file_result.get('run_id', '')

            # Get file modification timestamp (Unix timestamp from result-summary file)
            # This ensures date filtering is based on when the result-summary file was created,
            # not when the report.json was generated. Falls back to report timestamp for backward compatibility.
            file_modified = file_result.get('file_modified')
            file_timestamp = None
            if file_modified:
                # Convert Unix timestamp to ISO format for consistency
                from datetime import datetime
                try:
                    dt = datetime.fromtimestamp(file_modified)
                    file_timestamp = dt.isoformat()
                except (ValueError, OSError):
                    # If conversion fails, fall back to report timestamp
                    file_timestamp = report_timestamp
            else:
                # Fall back to report generation timestamp if file_modified not available
                # This maintains backward compatibility with older JSON reports
                file_timestamp = report_timestamp

            # Ensure these are dictionaries, not strings
            common_params = file_result.get('common_params', {})
            if not isinstance(common_params, dict):
                common_params = {}

            key_tags = file_result.get('key_tags', {})
            if not isinstance(key_tags, dict):
                key_tags = {}

            iterations = file_result.get('iterations', [])
            if not isinstance(iterations, list):
                iterations = []

            for iteration in iterations:
                # Skip if not a dictionary
                if not isinstance(iteration, dict):
                    continue

                iteration_id = iteration.get('iteration_id', '')

                unique_params = iteration.get('unique_params', {})
                if not isinstance(unique_params, dict):
                    unique_params = {}

                results_list = iteration.get('results', [])
                if not isinstance(results_list, list):
                    results_list = []

                # Process each result in the iteration
                # (there can be multiple results per iteration)
                for result in results_list:
                    # Skip if not a dictionary
                    if not isinstance(result, dict):
                        continue
                    # Get datapath model
                    datapath_model = key_tags.get('model')

                    # Get NIC from tags, or fallback to lab_info mapping
                    nic_value = key_tags.get('nic')
                    if not nic_value and datapath_model and datapath_model in nic_mapping:
                        nic_value = nic_mapping[datapath_model]

                    bench_result = BenchmarkResult(
                        regulus_data=regulus_data,
                        benchmark=benchmark,
                        iteration_id=iteration_id,
                        run_id=run_id,
                        timestamp=file_timestamp,
                        report_source=report_source,

                        # Key tags
                        model=datapath_model,         # Datapath model
                        nic=nic_value,                # NIC vendor (from tags or lab_info)
                        arch=key_tags.get('arch'),    # CPU architecture
                        perf=key_tags.get('perf'),
                        offload=key_tags.get('offload'),
                        kernel=key_tags.get('kernel'),
                        rcos=key_tags.get('rcos'),
                        cpu=key_tags.get('cpu'),
                        topo=key_tags.get('topo'),
                        pods_per_worker=key_tags.get('pods-per-worker'),
                        scale_out_factor=key_tags.get('scale_out_factor'),

                        # Test parameters
                        test_type=unique_params.get('test-type'),
                        protocol=common_params.get('protocol'),
                        # nthreads can be in either unique_params or common_params
                        threads=self._parse_int(unique_params.get('nthreads') or common_params.get('nthreads')),
                        wsize=self._parse_int(unique_params.get('wsize')),
                        rsize=self._parse_int(unique_params.get('rsize')),

                        # Metrics
                        mean=result.get('mean'),
                        min=result.get('min'),
                        max=result.get('max'),
                        stddev=result.get('stddev'),
                        stddevpct=result.get('stddevpct'),
                        unit=result.get('type'),  # "Gbps", "rx-Gbps", etc.
                        busy_cpu=result.get('busyCPU'),
                        samples_count=result.get('samples_count')
                    )

                    benchmark_results.append(bench_result)

        return benchmark_results

    def extract_all_results(self) -> List[BenchmarkResult]:
        """
        Extract benchmark results from all loaded reports.

        Returns:
            Combined list of all benchmark results across all reports
        """
        all_results = []
        for i, report in enumerate(self.loaded_reports):
            # Get report source from metadata if available
            report_source = self.metadata[i].regulus_data if i < len(self.metadata) else None
            results = self.extract_benchmark_results(report, report_source=report_source)
            all_results.extend(results)
        return all_results

    def get_summary_stats(self) -> Dict[str, Any]:
        """
        Get summary statistics across all loaded reports.

        Returns:
            Dictionary with summary statistics
        """
        if not self.metadata:
            return {
                'total_reports': 0,
                'total_files': 0,
                'total_iterations': 0,
                'benchmarks': [],
                'date_range': None
            }

        all_benchmarks = set()
        for meta in self.metadata:
            all_benchmarks.update(meta.benchmarks)

        # Parse timestamps and find date range from report metadata
        timestamps = [meta.timestamp for meta in self.metadata if meta.timestamp]
        date_range = None
        if timestamps:
            # Sort timestamps to get earliest and latest
            sorted_ts = sorted(timestamps)
            date_range = {
                'earliest': sorted_ts[0],
                'latest': sorted_ts[-1]
            }

        return {
            'total_reports': len(self.loaded_reports),
            'total_files': sum(meta.total_files for meta in self.metadata),
            'total_iterations': sum(meta.total_iterations for meta in self.metadata),
            'benchmarks': sorted(list(all_benchmarks)),
            'date_range': date_range
        }

    def _parse_int(self, value: Any) -> Optional[int]:
        """Safely parse integer values."""
        if value is None:
            return None
        try:
            return int(value)
        except (ValueError, TypeError):
            return None


class ReportFilter:
    """Filter benchmark results based on various criteria."""

    @staticmethod
    def filter_by_benchmark(results: List[BenchmarkResult], benchmark: str) -> List[BenchmarkResult]:
        """Filter results by benchmark type. Supports comma-separated values."""
        if ',' in benchmark:
            # Multiple values: split and match any
            values = [v.strip() for v in benchmark.split(',')]
            return [r for r in results if r.benchmark in values]
        else:
            # Single value
            return [r for r in results if r.benchmark == benchmark]

    @staticmethod
    def filter_by_tag(results: List[BenchmarkResult], tag_name: str, tag_value: str) -> List[BenchmarkResult]:
        """Filter results by a specific tag (model, kernel, etc.). Supports comma-separated values."""
        # Integer fields that need type conversion
        int_fields = {'threads', 'wsize', 'rsize'}

        if ',' in tag_value:
            # Multiple values: split and match any
            values = [v.strip() for v in tag_value.split(',')]
            if tag_name in int_fields:
                # Convert to integers for comparison
                try:
                    int_values = [int(v) for v in values]
                    return [r for r in results if getattr(r, tag_name, None) in int_values]
                except ValueError:
                    return []
            else:
                return [r for r in results if getattr(r, tag_name, None) in values]
        else:
            # Single value
            if tag_name in int_fields:
                # Convert to integer for comparison
                try:
                    int_value = int(tag_value)
                    return [r for r in results if getattr(r, tag_name, None) == int_value]
                except ValueError:
                    return []
            else:
                return [r for r in results if getattr(r, tag_name, None) == tag_value]

    @staticmethod
    def filter_by_date_range(results: List[BenchmarkResult], start_date: str, end_date: str) -> List[BenchmarkResult]:
        """Filter results by timestamp range."""
        try:
            start = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
            end = datetime.fromisoformat(end_date.replace('Z', '+00:00'))

            filtered = []
            for r in results:
                if r.timestamp:
                    try:
                        ts = datetime.fromisoformat(r.timestamp.replace('Z', '+00:00'))
                        if start <= ts <= end:
                            filtered.append(r)
                    except:
                        continue
            return filtered
        except:
            return results

    @staticmethod
    def get_unique_values(results: List[BenchmarkResult], field: str) -> List[str]:
        """Get unique values for a specific field across all results."""
        values = set()
        for r in results:
            val = getattr(r, field, None)
            if val is not None:
                values.add(str(val))
        return sorted(list(values))

    @staticmethod
    def filter_by_nic(results: List[BenchmarkResult], nic: str) -> List[BenchmarkResult]:
        """Filter results by NIC vendor."""
        return [r for r in results if r.nic == nic]

    @staticmethod
    def filter_by_days_ago(results: List[BenchmarkResult], days: int) -> List[BenchmarkResult]:
        """Filter results to only include data from the last N days."""
        from datetime import datetime, timedelta
        import re

        # Calculate cutoff date
        cutoff = datetime.now() - timedelta(days=days)

        filtered = []
        for r in results:
            if r.timestamp:
                try:
                    # Parse timestamp - handle multiple formats
                    # Remove timezone suffix for parsing
                    ts_str = r.timestamp.replace('Z', '').replace('+00:00', '')

                    # Try parsing with microseconds first
                    try:
                        ts = datetime.strptime(ts_str, '%Y-%m-%dT%H:%M:%S.%f')
                    except ValueError:
                        # Try without microseconds
                        ts = datetime.strptime(ts_str, '%Y-%m-%dT%H:%M:%S')

                    # Include if within the date range
                    if ts >= cutoff:
                        filtered.append(r)
                except Exception as e:
                    # If timestamp parsing fails, skip this result
                    continue

        return filtered
